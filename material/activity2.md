# Activity 1


> [!IMPORTANT]  
> During the activity, you are encouraged to question every line of code. Feel free to use your preferred LLM to assist with code-related inquiries.


----


In this activity, your group will **follow the instructions in the notebook** below to implement a Byte Pair Encoding (BPE) tokenizer from scratch:  

ðŸ“Œ **Notebook Link:** [Build a BPE Tokenizer from Scratch](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb)  

### **Discussion Questions**  
As you work through the notebook, discuss the following questions within your group to deepen your understanding of tokenizers and Byte Pair Encoding:  

1. **What is the main goal of a tokenizer in NLP models?**  
2. **How does Byte Pair Encoding (BPE) differ from traditional word-based tokenization?**  
3. **Why does BPE merge frequent character pairs instead of splitting words?**  
4. **What are the advantages of using BPE in models like GPT?**  
5. **Can you think of any limitations or drawbacks of BPE? How do modern tokenization techniques (like WordPiece or Unigram) address these?**  
6. **Look at the tokenized output generated in the notebook. Can you find any interesting patterns in how words are split?**  
7. **How does BPE handle out-of-vocabulary (OOV) words?**  


### **Instructions**  
- Work together as a group to complete the notebook.  
- Discuss the questions as you go along and write down key insights.  
- Donâ€™t hesitate to experimentâ€”modify the code, test different texts, and analyze the results!  
- If needed, use an LLM (such as ChatGPT) to help clarify concepts or troubleshoot issues.  

By the end of this activity, you should have a good understanding of how Byte Pair Encoding works and why it is widely used in NLP models. 

Happy coding! 