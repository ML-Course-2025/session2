# Session 2: Datasets


> [!NOTE]  
> - [How to share Your Colab Notebook](./material/colab.md)
> - [Zoom Recordings](https://metropoliafi-my.sharepoint.com/:f:/g/personal/samiben_metropolia_fi/EuSPkRmWcYpGsXdjFhE2k80BaZBR-EeccL7AHlnTQya-6w)
> - [The EU Artificial Intelligence Act](https://artificialintelligenceact.eu/)
> - More on the project later.

<!-- Please [share your Colab notebooks with the following email address](./material/colab.md):  *ml.course.2025* *[a]* *gmail* -->
---
### Topics

- Datasets for Classical Machine Learning
- Datasets for Large Language Models (LLMs)

---

### Tentative Timeline 

- Mini lecture
- `Group Activity + Break` (50min)  
- Mini lecture
- Group Activity

---

> [!IMPORTANT]  
> [Homework for this week](./material/homework.md)


-----

#### Part 0: Introduction to Data and Datasets for ML and LLMs

- Open source LLMs?
  - [Sky-T1](https://github.com/NovaSky-AI/SkyThought)
  - [Reproduction of DeepSeek-R1](https://github.com/huggingface/open-r1)
  - [OLMo 2](https://allenai.org/blog/olmo2-32B)
- ML systems vs Traditional programming
  - Importance of Data
  - [From Data to rules](./material/ml_vs_traditional_paradigm.png)
  - [Classical ML to DL to LLM](./material/ml2llm.png)
- [Summary](./material/part0.md)

---

### Part 1: 

- Data and Datasets for Classical Machine Learning
  - Data Cleaning and Preprocessing
  - Exploratory Data Analysis (EDA)
  - Feature Engineering
  - Data Visualization Techniques 
- [Summary: Key Points and Takeaways](./material/part1.md)
- [Activity 1: Practical Exercises on Data Cleaning and Preprocessing](./material/activity1.md)

---

### Part 2: 

- Datasets for Large Language Models (LLMs)
  - Data Sources for LLMs
  - Data Collection Strategies for LLMs
  - Data Preprocessing and Tokenization Techniques
  - Challenges in Data Collection and Curation for LLMs
- [Summary: Key Concepts from Part 2](./material/part2.md)
- [Activity 2: Practical Exploration of LLM Dataset Preparation](./material/activity2.md)


----
> [!TIP]  
> For those trying to study BPE, [here is an exercise](https://github.com/karpathy/minbpe/blob/master/exercise.md) on how you can build your own your own GPT-4 tokenizer. There is also a sample sample solution [here](https://github.com/karpathy/minbpe).





<!-- 

> [!NOTE]  
> Highlights information that users should take into account, even when skimming.

> [!TIP]
> Optional information to help a user be more successful.

> [!IMPORTANT]  
> Crucial information necessary for users to succeed.

> [!WARNING]  
> Critical content demanding immediate user attention due to potential risks.

> [!CAUTION]
> Negative potential consequences of an action. 

-->